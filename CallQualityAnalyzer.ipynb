{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYLY3Yf_RzbT"
   },
   "outputs": [],
   "source": [
    "# Core tools for ASR, Diarization, NLP, and audio analysis\n",
    "!pip install yt-dlp whisper transformers torch librosa soundfile pydub --quiet\n",
    "!pip install pyannote.audio --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKL_z4zubfOY"
   },
   "source": [
    "# Core tools for ASR, Diarization, NLP, and Audio Analysis\n",
    "\n",
    "```python\n",
    "# Install youtube-dl fork (yt-dlp) for downloading audio/video content\n",
    "# Install whisper (OpenAI's Automatic Speech Recognition model for speech-to-text)\n",
    "# Install transformers (Hugging Face library for NLP models like summarization, translation, etc.)\n",
    "# Install torch (PyTorch backend required for deep learning models)\n",
    "# Install librosa (used for audio feature extraction and analysis)\n",
    "# Install soundfile (for reading and writing audio files)\n",
    "# Install pydub (for audio manipulation: cutting, merging, format conversion)\n",
    "!pip install yt-dlp whisper transformers torch librosa soundfile pydub --quiet\n",
    "\n",
    "# Install pyannote.audio (library for speaker diarization, speaker embedding, and related tasks)\n",
    "!pip install pyannote.audio --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yncu3EO5b2In"
   },
   "source": [
    "This block uses **yt-dlp** to download audio from a YouTube video.\n",
    "\n",
    "- `url` defines the YouTube video link.\n",
    "- `output_file` sets the desired filename for the extracted audio.\n",
    "- `ydl_opts` is a dictionary of options for `yt-dlp`:\n",
    "  - `'format': 'bestaudio/best'` → fetch the best available audio quality.\n",
    "  - `'outtmpl': 'sales_call.mp3'` → save the audio file as `sales_call.mp3`.\n",
    "  - `'quiet': False` → display progress and logs during download.\n",
    "  - `'postprocessors'` → after downloading, use **FFmpeg** to extract audio:\n",
    "    - Convert to `mp3` format.\n",
    "    - Set audio quality to **192 kbps**.\n",
    "  - `'keepvideo': True` → retain the original video to avoid filename conflicts.\n",
    "- The `with yt_dlp.YoutubeDL(ydl_opts)` block executes the download process.\n",
    "- After downloading, `os.path.exists` checks if the file was successfully created.\n",
    "- If successful, it prints the file size; otherwise, it logs an error message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9475,
     "status": "ok",
     "timestamp": 1756462453521,
     "user": {
      "displayName": "Ayesha Shaik",
      "userId": "11468338137185894966"
     },
     "user_tz": -330
    },
    "id": "2dj-gZqDR4pE",
    "outputId": "71e7e866-092d-41ad-f430-915e8790eb09"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yt_dlp\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=4ostqJD3Psc\"\n",
    "output_file = \"sales_call.mp3\"\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'outtmpl': 'sales_call.mp3',\n",
    "    'quiet': False,\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'mp3',\n",
    "        'preferredquality': '192',\n",
    "    }],\n",
    "    'keepvideo': True  # Prevent deleting original to avoid filename confusion\n",
    "}\n",
    "\n",
    "# Attempt download and print file size after\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([url])\n",
    "\n",
    "print(\"Download complete?\", os.path.exists(output_file))\n",
    "if os.path.exists(output_file):\n",
    "    print(\"File size:\", os.path.getsize(output_file))\n",
    "else:\n",
    "    print(\"ERROR: Audio file not downloaded. Check yt-dlp output above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caS2ITlIb9jT"
   },
   "source": [
    "This block uses **pydub** to convert an MP3 audio file to WAV format for further analysis.\n",
    "\n",
    "- `from pydub import AudioSegment` → imports the main class for audio manipulation.\n",
    "- `AudioSegment.from_mp3(\"sales_call.mp3\")` → loads the MP3 file into an `AudioSegment` object.\n",
    "- `wav_output = \"sales_call.wav\"` → sets the output filename for the WAV file.\n",
    "- `audio.export(wav_output, format=\"wav\")` → exports/converts the audio to WAV format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1231,
     "status": "ok",
     "timestamp": 1756462562715,
     "user": {
      "displayName": "Ayesha Shaik",
      "userId": "11468338137185894966"
     },
     "user_tz": -330
    },
    "id": "-W8F21sFSEJ8",
    "outputId": "c9106cf1-6bc8-42f4-ca82-f4f11088960a"
   },
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "# Convert to WAV for analysis\n",
    "audio = AudioSegment.from_mp3(\"sales_call.mp3\")\n",
    "wav_output = \"sales_call.wav\"\n",
    "audio.export(wav_output, format=\"wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 16407,
     "status": "ok",
     "timestamp": 1756462361825,
     "user": {
      "displayName": "Ayesha Shaik",
      "userId": "11468338137185894966"
     },
     "user_tz": -330
    },
    "id": "8G15CeoJUGIo",
    "outputId": "3dd4fd6f-dabd-4449-b175-5c88abebbf33"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y whisper\n",
    "!pip install git+https://github.com/openai/whisper.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCqzXRqQcGt3"
   },
   "source": [
    "This block uses **OpenAI Whisper** to transcribe audio into text and extract time-stamped segments.\n",
    "\n",
    "- `import whisper` → imports the Whisper library for automatic speech recognition (ASR).\n",
    "- `model = whisper.load_model(\"small\")` → loads the small Whisper model (faster, moderate accuracy).\n",
    "- `result = model.transcribe(wav_output)` → transcribes the WAV audio file and returns a dictionary containing the transcript and segment details.\n",
    "- `transcript = result[\"text\"]` → extracts the full transcribed text.\n",
    "- `segments = result[\"segments\"]` → extracts a list of time-stamped segments, each containing start/end times and the corresponding spoken text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZaP4m7MaSsNU"
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"small\")\n",
    "result = model.transcribe(wav_output)\n",
    "transcript = result[\"text\"]\n",
    "segments = result[\"segments\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8N8-7dY_cNSh"
   },
   "source": [
    "This block uses **PyAnnote Audio** to perform speaker diarization (identifying who speaks when) on the audio.\n",
    "\n",
    "- `from pyannote.audio import Pipeline` → imports the pipeline class for audio processing tasks like diarization.\n",
    "- `HUG_AUTH_TOKEN = \"...\"` → your Hugging Face authentication token required to access pretrained models.\n",
    "- `Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=HUG_AUTH_TOKEN)` → loads the pretrained speaker diarization model version 3.1.\n",
    "- `diarization = pipeline(wav_output)` → applies the diarization model to the WAV file, producing speaker segments with start/end times.\n",
    "- The `for` loop iterates over the diarization output using `itertracks(yield_label=True)`:\n",
    "  - Extracts `start` and `end` times for each speaker segment.\n",
    "  - Extracts the `speaker` label.\n",
    "  - Appends a dictionary for each turn with `{'start': turn.start, 'end': turn.end, 'speaker': str(speaker)}` for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "94f0249c6a2842a68c0702140ffb26eb",
      "83b9adb2961849198a8026843a1b61ad",
      "cb1720b63edf402f8b38341053446aa2",
      "cde3757a16b342d09f2e253f59997cb4",
      "4f71ea0390be4f66986f394c0af1958c",
      "2601f7dcf27c4631bad16861f67e74b0",
      "d216c52f03184be6980d634c95d5c144",
      "76906f32798145ca9e22c297609de8d5",
      "72a1c1e902b742ef8387b39b66807143",
      "247f542303584cac81a049bb07a8a482",
      "8e618c0723fc4a45a6cda270ad752632",
      "8353e24b2a9146c58ab2af3a9af2bd86",
      "0e02a0a974264385b8b9f9f688ef868c",
      "c9880c84acde4f07a331c8b21122248e",
      "feca59ebf3b44eddbbf5496df7369e13",
      "fad8cb6ef06049a084dac083e8fd4404",
      "f9e4a0fbd8a34471991799807c69b3f7",
      "dc5fcc7669a84a39968ff12ecfbeeb6d",
      "7c29f133bd044e9d9637bcd399325eab",
      "5329600fe2f049d4ae7b400d863c498b",
      "f986e17777834791897d97601c15e133",
      "609caecbcbcf45d8becfcaa904523532",
      "095247648f994d7190d808cd29f543df",
      "d588a5e368d7407fa1167e25423f088b",
      "78d75d3fff114d2db4d159cef9ba66ec",
      "a6db3f8b75b94827bd90069ff6902968",
      "d4458d7a4c8b47579eefef66c10aa6e9",
      "e44d531bd8cb4269ab6d14f9becc139f",
      "42b4b167224b49c78319d686d4e21202",
      "746366e08cd84f7a97d43495833861ec",
      "a8991c8fb30244fa9a027240bb012c4b",
      "a3fdf686784644b08d1aafb752285b47",
      "dabfb88c3af4471db56deca7a6cc515a",
      "2a99eda72258416ba7f47508260cbb1a",
      "7cdf341b19a2462c8c3eae57ba336211",
      "3572c4acca054f30ae30deb37c9a5888",
      "c568403b777548b096c9fe2def0c6593",
      "23040827b4ea40b1905d77ac5db660a6",
      "2f7b6afcd25d416dab3e60149c6cf90b",
      "49d24f7869c24e73b26ed9f3a6d9b180",
      "4461dc7c6bf544f488faf46cf4645b1a",
      "e9f28dc532ba4ccaa142ea2b2f56185c",
      "e7b6520ed195492590c8d29e42d2fc77",
      "3733f0f91926480cb5a274889137cbd5"
     ]
    },
    "executionInfo": {
     "elapsed": 309244,
     "status": "ok",
     "timestamp": 1756463660400,
     "user": {
      "displayName": "Ayesha Shaik",
      "userId": "11468338137185894966"
     },
     "user_tz": -330
    },
    "id": "jsre9Dl_T29U",
    "outputId": "ebe32f22-85cd-4d5a-d077-6a2137c0d830"
   },
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "\n",
    "# You will need a free HuggingFace token: https://hf.co/settings/tokens\n",
    "HUG_AUTH_TOKEN = \"HUG_AUTH_TOKEN \"\n",
    "\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=HUG_AUTH_TOKEN\n",
    ")\n",
    "diarization = pipeline(wav_output)\n",
    "\n",
    "# Create turns per speaker for analysis\n",
    "diarization_turns = []\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    diarization_turns.append({'start': turn.start, 'end': turn.end, 'speaker': str(speaker)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bHjbOhjcVv3"
   },
   "source": [
    "This block matches each transcribed segment to the most likely speaker using the diarization results.\n",
    "\n",
    "- `import numpy as np` → imports NumPy (not strictly needed here, but useful for numerical operations if needed later).\n",
    "- `assign_speaker(segment, diarization_turns)` → a helper function that assigns a speaker to a segment:\n",
    "  - Calculates the midpoint of the segment: `mid = (segment['start'] + segment['end']) / 2`.\n",
    "  - Iterates over all `diarization_turns` and checks which turn contains the midpoint.\n",
    "  - Returns the corresponding speaker label. If no match is found, returns `\"Unknown\"`.\n",
    "- The `utterances` list is created by iterating over all `segments` from Whisper:\n",
    "  - Calls `assign_speaker` for each segment to determine the speaker.\n",
    "  - Appends a dictionary with `'text'`, `'start'`, `'end'`, and `'speaker'` for each utterance.\n",
    "- Result: `utterances` now contains time-stamped, speaker-attributed transcript segments ready for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVeFzBT6XqWO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Assign most likely speaker to each segment by matching times\n",
    "def assign_speaker(segment, diarization_turns):\n",
    "    mid = (segment['start'] + segment['end']) / 2\n",
    "    for turn in diarization_turns:\n",
    "        if turn['start'] <= mid <= turn['end']:\n",
    "            return turn['speaker']\n",
    "    return \"Unknown\"\n",
    "\n",
    "utterances = []\n",
    "for seg in segments:\n",
    "    speaker = assign_speaker(seg, diarization_turns)\n",
    "    utterances.append({'text': seg['text'], 'start': seg['start'], 'end': seg['end'], 'speaker': speaker})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4N-XcnUwcf8g"
   },
   "source": [
    "This block calculates **talk-time ratios** and identifies the **longest monologue** per speaker.\n",
    "\n",
    "- `from collections import defaultdict` → imports a dictionary that returns a default value for missing keys.\n",
    "- `talk_times = defaultdict(float)` → stores cumulative speaking time for each speaker.\n",
    "- `monologues = defaultdict(float)` → stores the duration of the longest single speech (monologue) per speaker.\n",
    "- The `for utt in utterances` loop:\n",
    "  - Computes the duration of each utterance: `duration = utt['end'] - utt['start']`.\n",
    "  - Adds the duration to the speaker’s total in `talk_times`.\n",
    "  - Updates `monologues` if this utterance is longer than the previous longest.\n",
    "- `total_time = segments[-1]['end'] - segments[0]['start']` → computes total audio duration from the first segment start to last segment end.\n",
    "- `ratios = {spk: round(100 * t / total_time, 2) for spk, t in talk_times.items()}` → calculates each speaker’s talk-time as a percentage of the total audio.\n",
    "- `print(\"Talk-time Ratios (%):\", ratios)` → shows the percentage of time each speaker talked.\n",
    "- `print(\"Longest Monologue (sec):\", monologues)` → shows the duration of the longest single speech for each speaker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1756463976419,
     "user": {
      "displayName": "Ayesha Shaik",
      "userId": "11468338137185894966"
     },
     "user_tz": -330
    },
    "id": "nd8qcPPfWEAS",
    "outputId": "2df74f9b-5f2b-4cc4-c6dd-cf71aa00a616"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "talk_times = defaultdict(float)\n",
    "monologues = defaultdict(float)\n",
    "\n",
    "for utt in utterances:\n",
    "    duration = utt['end'] - utt['start']\n",
    "    talk_times[utt['speaker']] += duration\n",
    "    monologues[utt['speaker']] = max(monologues[utt['speaker']], duration)\n",
    "\n",
    "# Fix: Use 0 for first element instead of 'start'\n",
    "total_time = segments[-1]['end'] - segments[0]['start']\n",
    "\n",
    "ratios = {spk: round(100 * t / total_time, 2) for spk, t in talk_times.items()}\n",
    "\n",
    "print(\"Talk-time Ratios (%):\", ratios)\n",
    "print(\"Longest Monologue (sec):\", monologues)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0JBYU86cois"
   },
   "source": [
    "This block detects **questions** in the transcript and counts how many each speaker asked.\n",
    "\n",
    "- `from transformers import pipeline` → imports Hugging Face pipeline for NLP tasks.\n",
    "- `nlp = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")` → loads a sentiment analysis model (not directly used for question detection here, could be used later for sentiment of questions/answers).\n",
    "- `is_question(text)` → helper function to identify questions:\n",
    "  - Checks if the text contains a `?`.\n",
    "  - Checks if the text starts with common interrogative words like `what`, `how`, `why`, `when`, `where`, `which`, `who`, `whom`, `whose`.\n",
    "- `questions = defaultdict(int)` → initializes a counter for questions per speaker.\n",
    "- The `for utt in utterances` loop:\n",
    "  - Calls `is_question` on each utterance.\n",
    "  - If true, increments the count for the corresponding speaker.\n",
    "- `print(\"Questions Asked: \", dict(questions))` → displays the total number of questions asked by each speaker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304,
     "referenced_widgets": [
      "714ce04825274fa58850f01958dc8310",
      "15cf8d1817d245f3a7fdfce64a2fb548",
      "081aff425e4544378b7b989855a5a480",
      "638a9f5b451449998cff201782b57abe",
      "3243f0dff35a4cadb82849f6a1bd1b5b",
      "17fd321dc334486794ac42a77a58ecc7",
      "339b72407bde4d20b03366afa1c6cc9a",
      "7b99167f866240dba6646f483880c6e6",
      "6a69a00c87404db79862bf7429336a4f",
      "40304cf9aa0a40e8a9a52131a7be6be0",
      "5529f0b577764242b483b28ee68b5a34",
      "848d86f9402f48c9aaad8ddc69a8adeb",
      "b2b02f20719341e09346daab9f63d3bd",
      "1906bc7020864497bacfb74e3cd89b20",
      "feca406d968a4bde9b0bc604324a8847",
      "ad2815575d9840e6bf4f6dde4eb2f500",
      "986d3991b1804b51b6f4f07fda2e02f8",
      "a5072b9ef87041b58e608c6e0a264c00",
      "17fbe8482c7d4ec691da24047e99873a",
      "d22aba5e75ce499bbbcc272dfcdcb08e",
      "288087d85a9248c0a005402371cf4eb9",
      "17cb1651a13c4e31b4f4d59254e4bfd3",
      "1a76517802c647409bba75ca76505201",
      "33753e96cde8483a8829e74e4838adec",
      "25b06e346a124172828586f599cb77b9",
      "3080bf093a474a7a9d8a9c3e0f3a6150",
      "b954d0c2782845ea87f7c97fb13d87e9",
      "2994d6a1a97f4c36961264549e1a61cb",
      "d5f3e552f516480094be956ba0468bcb",
      "1b662bb589964db1afe8c2b910c67b92",
      "773bea004421443ebc395a2b69ad5b1c",
      "15294953aad9411c94973ee692e5f8c2",
      "a4e85fcf597f41468f7395b6df59de91",
      "e39d280de8fe4af7bb9f94dff785eaad",
      "2ca7e9ad2fc544a2830e345f48a3b51d",
      "3a7b4ef64310438cbcf3357a3a029f0f",
      "3b66b299689f49579462e4e5592ce535",
      "d818acb1dfc4471da8649da8ac36d4ec",
      "3c0903ff1d454960a3f71b4dd6b02235",
      "4d9e6c63387044c89a8b22bc01ca72cc",
      "a0f315eaf9564fe2985879c6be3aaf69",
      "5115542990bb407faf2a1e278978f760",
      "367ea9f69ea24d238f5a4778532da879",
      "a96393ff3dd44317bfce4794ad748230"
     ]
    },
    "executionInfo": {
     "elapsed": 17036,
     "status": "ok",
     "timestamp": 1756464008191,
     "user": {
      "displayName": "Ayesha Shaik",
      "userId": "11468338137185894966"
     },
     "user_tz": -330
    },
    "id": "SSOfNY7uaIQI",
    "outputId": "7a3eb61d-50e4-485a-9fc3-84c0f37d6653"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "def is_question(text):\n",
    "    # Simple: question mark or frequent interrogatives\n",
    "    question_words = ['what', 'how', 'why', 'when', 'where', 'which', 'who', 'whom', 'whose']\n",
    "    if '?' in text:\n",
    "        return True\n",
    "    return any(text.lower().startswith(qw) for qw in question_words)\n",
    "\n",
    "questions = defaultdict(int)\n",
    "for utt in utterances:\n",
    "    if is_question(utt['text']):\n",
    "        questions[utt['speaker']] += 1\n",
    "print(\"Questions Asked: \", dict(questions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PNTFPJecvj8"
   },
   "source": [
    "This block performs **sentiment analysis** on both the transcript text and audio features.\n",
    "\n",
    "**Text-Based Sentiment:**\n",
    "- `full_text = \" \".join([utt['text'] for utt in utterances])` → combines all utterances into a single string for analysis.\n",
    "- `sentiment = nlp(full_text[:512])` → applies a preloaded NLP pipeline (e.g., `distilbert-base-uncased-finetuned-sst-2-english`) to the first 512 characters for speed.\n",
    "- `print(\"Text-Based Sentiment: \", sentiment)` → outputs sentiment prediction (positive/negative) for the text.\n",
    "\n",
    "**Audio-Based Sentiment (simplified):**\n",
    "- `import librosa` and `import numpy as np` → load libraries for audio processing and numerical computations.\n",
    "- `y, sr = librosa.load(wav_output)` → loads the WAV audio file into a waveform array `y` with sampling rate `sr`.\n",
    "- `mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)` → extracts MFCC features (common in speech analysis) and averages them over time.\n",
    "- `audio_sentiment = \"neutral\"` → placeholder for audio-based sentiment; could be replaced with a trained emotion recognition model.\n",
    "- `print(\"Audio-Based Sentiment: \", audio_sentiment)` → outputs the simplified sentiment derived from audio features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12697,
     "status": "ok",
     "timestamp": 1756464051560,
     "user": {
      "displayName": "Ayesha Shaik",
      "userId": "11468338137185894966"
     },
     "user_tz": -330
    },
    "id": "L9WLpQ33aYfm",
    "outputId": "321dd946-c36a-47c0-8266-a32fcbc4a6a3"
   },
   "outputs": [],
   "source": [
    "# Text-based sentiment\n",
    "full_text = \" \".join([utt['text'] for utt in utterances])\n",
    "sentiment = nlp(full_text[:512])  # Truncate for speed\n",
    "print(\"Text-Based Sentiment: \", sentiment)\n",
    "\n",
    "# Audio-based (use MFCC features, requires ML model)\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "y, sr = librosa.load(wav_output)\n",
    "mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n",
    "\n",
    "# Simulate audio sentiment (extend with emotion recognition model for depth)\n",
    "audio_sentiment = \"neutral\" # For simplicity; replace with trained classifier for true emotion detection\n",
    "print(\"Audio-Based Sentiment: \", audio_sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMvxPSfFc2Pz"
   },
   "source": [
    "This block generates a **concise summary** of the transcript to capture main topics or actionable insights.\n",
    "\n",
    "- `from transformers import pipeline` → imports the Hugging Face pipeline for NLP tasks.\n",
    "- `summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")` → loads a pretrained summarization model.\n",
    "- `summary = summarizer(full_text[:1024])[0]['summary_text']` → summarizes the first 1024 characters of the transcript and extracts the text from the returned list of dictionaries.\n",
    "- `print(\"Actionable Insight (summary):\", summary)` → outputs the summarized text highlighting key points or main topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4234,
     "status": "ok",
     "timestamp": 1756464134327,
     "user": {
      "displayName": "Ayesha Shaik",
      "userId": "11468338137185894966"
     },
     "user_tz": -330
    },
    "id": "bqRTbgnlakIM",
    "outputId": "55cbfb60-281f-4cb1-f59a-478c4671e795"
   },
   "outputs": [],
   "source": [
    "# Simple: Summarize main topics/questions\n",
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "summary = summarizer(full_text[:1024])[0]['summary_text']\n",
    "print(\"Actionable Insight (summary):\", summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-GM1Lylc8df"
   },
   "source": [
    "This block uses a simple heuristic to **identify likely roles** (sales rep vs. customer) based on question frequency.\n",
    "\n",
    "- `likely_rep = max(questions, key=questions.get)` → assumes the speaker who asked the most questions is the sales representative.\n",
    "- `likely_customer = min(questions, key=questions.get)` → assumes the speaker who asked the fewest questions is the customer.\n",
    "- `print(f\"Likely Sales Rep: {likely_rep}\")` → displays the speaker likely to be the sales rep.\n",
    "- `print(f\"Likely Customer: {likely_customer}\")` → displays the speaker likely to be the customer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1756464147164,
     "user": {
      "displayName": "Ayesha Shaik",
      "userId": "11468338137185894966"
     },
     "user_tz": -330
    },
    "id": "gI2FI4xCaot6",
    "outputId": "80b3ddbc-47d7-4210-ff7b-168200fbfbb1"
   },
   "outputs": [],
   "source": [
    "# Heuristic: Speaker asking more questions likely sales rep\n",
    "likely_rep = max(questions, key=questions.get)\n",
    "likely_customer = min(questions, key=questions.get)\n",
    "print(f\"Likely Sales Rep: {likely_rep}\")\n",
    "print(f\"Likely Customer: {likely_customer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-b_MH0-dDAp"
   },
   "source": [
    "This block **compiles all analysis results into a structured report** and prints it neatly.\n",
    "\n",
    "- `report = { ... }` → creates a dictionary containing:\n",
    "  - `\"talk_time_ratio (%)\"` → percentage of total talk time per speaker.\n",
    "  - `\"questions_asked\"` → number of questions asked by each speaker.\n",
    "  - `\"longest_monologue_duration (sec)\"` → duration of each speaker's longest monologue.\n",
    "  - `\"call_sentiment_text\"` → sentiment analysis result from the transcript text.\n",
    "  - `\"call_sentiment_audio\"` → (simplified) sentiment analysis result from the audio features.\n",
    "  - `\"actionable_insight\"` → summarized key points or topics from the call.\n",
    "  - `\"sales_rep_guess\"` → speaker likely identified as sales representative.\n",
    "  - `\"customer_guess\"` → speaker likely identified as customer.\n",
    "- `import pprint` → imports the pretty-print module for cleaner dictionary display.\n",
    "- `pprint.pprint(report)` → prints the report in a readable, formatted way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1756464160514,
     "user": {
      "displayName": "Ayesha Shaik",
      "userId": "11468338137185894966"
     },
     "user_tz": -330
    },
    "id": "YDS8tV4aa-k7",
    "outputId": "80f28a92-fda0-4102-f00d-b5875c377b14"
   },
   "outputs": [],
   "source": [
    "report = {\n",
    "    \"talk_time_ratio (%)\": ratios,\n",
    "    \"questions_asked\": dict(questions),\n",
    "    \"longest_monologue_duration (sec)\": dict(monologues),\n",
    "    \"call_sentiment_text\": sentiment,\n",
    "    \"call_sentiment_audio\": audio_sentiment,\n",
    "    \"actionable_insight\": summary,\n",
    "    \"sales_rep_guess\": likely_rep,\n",
    "    \"customer_guess\": likely_customer\n",
    "}\n",
    "import pprint\n",
    "pprint.pprint(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ol37bToldXVi"
   },
   "source": [
    "# Conversation Visualizations\n",
    "\n",
    "This block creates three types of visualizations to better understand the conversation dynamics.\n",
    "\n",
    "1. **Speaker Timelines (Gantt-style)**\n",
    "   - Shows who spoke when during the call.\n",
    "   - Each utterance is represented as a horizontal bar for the corresponding speaker.\n",
    "   - Distinct colors are used for each speaker.\n",
    "   - X-axis: time in seconds, Y-axis: speaker names.\n",
    "\n",
    "2. **Talk-Time Ratios (Pie Chart)**\n",
    "   - Displays each speaker's share of total talk time as a pie chart.\n",
    "   - Helps quickly identify which speaker dominated the conversation.\n",
    "\n",
    "3. **Questions Asked (Bar Chart)**\n",
    "   - Shows the number of questions asked by each speaker.\n",
    "   - X-axis: speakers, Y-axis: count of questions.\n",
    "   - Color-coded to match speaker timelines for consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1033,
     "status": "ok",
     "timestamp": 1756464760930,
     "user": {
      "displayName": "Ayesha Shaik",
      "userId": "11468338137185894966"
     },
     "user_tz": -330
    },
    "id": "YQ2KlV6ObByj",
    "outputId": "abbf14a9-051b-4f93-a254-628a127c078d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Speaker Timelines (Gantt-style)\n",
    "# ---------------------------\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "colors = ['skyblue', 'salmon', 'lightgreen', 'orange', 'purple']  # assign colors to speakers\n",
    "speaker_colors = {spk: colors[i % len(colors)] for i, spk in enumerate({utt['speaker'] for utt in utterances})}\n",
    "\n",
    "for utt in utterances:\n",
    "    ax.barh(\n",
    "        utt['speaker'],\n",
    "        width=utt['end'] - utt['start'],\n",
    "        left=utt['start'],\n",
    "        color=speaker_colors[utt['speaker']],\n",
    "        edgecolor='black'\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Time (sec)\")\n",
    "ax.set_ylabel(\"Speaker\")\n",
    "ax.set_title(\"Speaker Timelines (Who spoke when)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Talk-Time Ratios (Pie Chart)\n",
    "# ---------------------------\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(\n",
    "    list(ratios.values()),\n",
    "    labels=list(ratios.keys()),\n",
    "    autopct=\"%1.1f%%\",\n",
    "    colors=[speaker_colors[spk] for spk in ratios.keys()]\n",
    ")\n",
    "ax.set_title(\"Talk-Time Ratios\")\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Questions Asked (Bar Chart)\n",
    "# ---------------------------\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(\n",
    "    questions.keys(),\n",
    "    questions.values(),\n",
    "    color=[speaker_colors[spk] for spk in questions.keys()]\n",
    ")\n",
    "ax.set_xlabel(\"Speaker\")\n",
    "ax.set_ylabel(\"Number of Questions Asked\")\n",
    "ax.set_title(\"Questions Asked per Speaker\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xyf526pcdUHz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPMn8Q2EWy50TqxXzYt1MZF",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
